---
title: "Evaluación experimental"
author: "Irvin Rojas"
institute: "CIDE"
date: "24 de agosto de 2021"
header-includes:
  - \usepackage{tikz}
  - \usetikzlibrary{shapes, shadows,arrows}
output:
  xaringan::moon_reader:
    css: [default, "libs/cide.css", metropolis-fonts, "https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap-grid.min.css", "https://use.fontawesome.com/releases/v5.7.2/css/all.css", "https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css"]
    seal: false
    chakra: "https://remarkjs.com/downloads/remark-latest.min.js"
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      titleSlideClass: ["middle", "center"]
      ratio: "16:9"
      beforeInit: ["https://platform.twitter.com/widgets.js", "libs/cols_macro.js"]
      navigation:
        scroll: false


---
class: title-slide

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.path = "figures/")
library(tidyverse)
library(magick)
library(reticulate)
library(knitr)
library(kableExtra)
xfun::pkg_load2(c('base64enc', 'htmltools', 'mime'))
```

.title[
# Clase 3. Evaluación experimental
]
.subtitle[
## Evaluación de Programas
]
.author[
### Irvin Rojas <br> [rojasirvin.com](https://www.rojasirvin.com/) <br> [<i class="fab fa-github"></i>](https://github.com/rojasirvin) [<i class="fab fa-twitter"></i>](https://twitter.com/RojasIrvin) [<i class="ai ai-google-scholar"></i>](https://scholar.google.com/citations?user=FUwdSTMAAAAJ&hl=en)
]

.affiliation[
### Centro de Investigación y Docencia Económicas <br> División de Economía
]

---
# Agenda

---

---

class: inverse, middle, center

# Regresión para la evaluación

---

# Regresión como herramienta

Usaremos muy frecuentemente la regresión para la estimación de efectos causales

La regresión líneal es la herramienta estadística más usada en evaluación

La regresión nos servirá para hacer esencialmente lo mismo: comparación de $y_i$ entre grupos

Nos permitirá *controlar* por posibles factores que afecten $y$, haciendo más precisa la estimación del efecto del tratamiento

---

# Regresión como herramienta

.pull-left[
Consideremos este ejemplo hipotético

Existe una relación positiva entre los años de educación y el salario

```{r echo=FALSE, message=FALSE, warnings=FALSE, results=F}
set.seed(1234)
##Linear Regression
#Generate the independent variable and the error
educacion <-  rnorm(100,10,3)
e <-  rnorm(100,0,8)
#Generate the dependent variable (b0=150, b1=-4, b2=2.5)
salario <- 100 + 2 * educacion + e 
datos <- as.data.frame(cbind(salario, educacion))
#create the model
m1=lm(salario ~ educacion)
summary(m1)
```
]
.pull-right[
```{r echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
datos <- augment(m1)
datos %>% 
  ggplot(aes(x=educacion, y=salario )) +
  geom_point()+
  labs(x="Educación", "Salario por hora")
```
]

---

# El principio de mínimos cuadrados


.pull-left[
Existe una infinidad de líneas que puedo trazar intentando describir los puntos

Cada línea está caracterizada por una ordenada al origen y una pendiente

$$\hat{y}=\alpha+\beta X$$

Esta recta describe el salario que esperaríamos dado un nivel de educación

A la diferencia entre lo observado y lo ajustado le llamamos **errores**

$$\hat{e}=y-\hat{y}$$


]

.pull-right[
```{r echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
datos %>% 
  ggplot(aes(x=educacion, y=salario )) +
  geom_point()+
  labs(x="Educación", "Salario por hora")
```
]


---

# El principio de mínimos cuadrados

.pull-left[
Yo puedo acumular el total de los errores, sumando simplemente $\sum_i \hat{e}_i$ para expresar una medida de discrepancia entre mi recta y los puntos observados

Pero también me gustaría hacer que los errores más grandes *pesen* más que los más errores más pequeños

$$L=\sum_i \hat{e}_i$$
Sustituyendo lo que vale $\hat{e}$ y $\hat{y}$

$$L=\sum_i \hat{e}_i=\sum_i (y_i-\hat{y}_i)^2=\sum_i (y_i-\alpha-\beta X_i)^2$$




]


.pull-right[
```{r echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
datos %>% 
  ggplot(aes(x=educacion, y=salario )) +
  geom_point()+
  labs(x="Educación", "Salario por hora")
```
]


---

# El principio de mínimos cuadrados

.pull-left[
El principio de MCO consiste en elegir el valor de $\alpha$ y $\beta$ que hagan $L$ lo más pequeño posible

En otras palabras, escogemos $\alpha$ y $\beta$ que minimicen la suma de los errores que cometemos al ajustar la recta a los puntos, penalizando los errores más grandes

A los valores de $\alpha$ y $\beta$ que minimizan la suma de los errores cuadráticos les conocemos como **estimadores de MCO**
]


.pull-right[
```{r echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
datos %>% 
  ggplot(aes(x=educacion, y=salario )) +
  geom_point()+
  labs(x="Educación", "Salario por hora")+
  geom_smooth(method = 'lm', se = F)+
  geom_segment(aes(xend = educacion, yend = .fitted), color = "red", size = 0.3)
``` 
]
---

# Regresión como herramienta


En la clase práctica veremos como en una regresión $\hat{\beta}$ será interpretada como la diferencia de la variable $y$ entre el grupo de tratamiento y el de control

Por tanto, podremos construir un estadístico $t$ y hacer pruebas de hipótesis basadas en el valor de $\hat{\beta}$ y su error estándar


---

# Próxima sesión

---

class: center, middle

Presentación creada usando el paquete [**xaringan**](https://github.com/yihui/xaringan) en R.

El *chakra* viene de [remark.js](https://remarkjs.com), [**knitr**](http://yihui.org/knitr), y [R Markdown](https://rmarkdown.rstudio.com).

Material de clase en versión preliminar.

**No reproducir, no distribuir, no citar.**


